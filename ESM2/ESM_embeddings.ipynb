{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ac67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from esm import Alphabet, FastaBatchedDataset, pretrained\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "!pip install -q fair-esm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import esm\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from esm import FastaBatchedDataset, pretrained\n",
    "\n",
    "def extract_embeddings(model_name, fasta_file, output_dir, tokens_per_batch=4096, seq_length=1022,repr_layers=[33]):\n",
    "    \n",
    "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    dataset = FastaBatchedDataset.from_file(fasta_file)\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=alphabet.get_batch_converter(seq_length), \n",
    "        batch_sampler=batches\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "\n",
    "            print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n",
    "\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]\n",
    "                \n",
    "                filename = output_dir / f\"{entry_id}.pt\"\n",
    "                truncate_len = min(seq_length, len(strs[i]))\n",
    "\n",
    "                result = {\"entry_id\": entry_id}\n",
    "                result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "\n",
    "                torch.save(result, filename)\n",
    "\n",
    "model_name = 'esm2_t33_650M_UR50D'\n",
    "fasta_file = pathlib.Path('../../first_run/BLAST/train_all_toxinpred3.fa')\n",
    "output_dir1 = pathlib.Path('train_embeddings')\n",
    "\n",
    "extract_embeddings(model_name, fasta_file, output_dir1)\n",
    "\n",
    "model_name = 'esm2_t33_650M_UR50D'\n",
    "fasta_file = pathlib.Path('../../first_run/BLAST//test_all_toxinpred3.fa')\n",
    "output_dir2 = pathlib.Path('test_embeddings')\n",
    "\n",
    "extract_embeddings(model_name, fasta_file, output_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d49613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###embeddings to feature_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5eb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "TRAIN_FASTA_PATH = \"../../../Downloads/TOXIC/first_run/BLAST/train_all_toxinpred3.fa\" # Path to P62593.fasta\n",
    "TRAIN_EMB_PATH = \"../../../Downloads/embeddings/train_embeddings_esm\" # Path to directory of embeddings for P62593.fasta\n",
    "EMB_LAYER = 33\n",
    "\n",
    "Xs_train = []\n",
    "for header, _seq in esm.data.read_fasta(TRAIN_FASTA_PATH):\n",
    "    fn = f'{TRAIN_EMB_PATH}/{header}.pt'\n",
    "    embs = torch.load(fn)\n",
    "    Xs_train.append(embs['mean_representations'][EMB_LAYER])\n",
    "Xs_train = torch.stack(Xs_train, dim=0).numpy()\n",
    "\n",
    "ys_train = pd.read_csv('train_seq1.csv')#['cnrci']\n",
    "print(len(ys_train))\n",
    "print(Xs_train.shape)\n",
    "\n",
    "#testing\n",
    "\n",
    "TEST_FASTA_PATH = \"../../../Downloads/TOXIC/first_run/BLAST/test_all_toxinpred3.fa\" # Path to P62593.fasta\n",
    "TEST_EMB_PATH = \"../../../Downloads/embeddings/test_embeddings_esm\" # Path to directory of embeddings for P62593.fasta\n",
    "EMB_LAYER = 33\n",
    "\n",
    "Xs_test = []\n",
    "for header, _seq in esm.data.read_fasta(TEST_FASTA_PATH):\n",
    "    fn = f'{TEST_EMB_PATH}/{header}.pt'\n",
    "    embs = torch.load(fn)\n",
    "    Xs_test.append(embs['mean_representations'][EMB_LAYER])\n",
    "Xs_test = torch.stack(Xs_test, dim=0).numpy()\n",
    "\n",
    "ys_test = pd.read_csv('test_seq.csv')#['cnrci']\n",
    "print(len(ys_test))\n",
    "print(Xs_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
